{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "930c2fd6",
   "metadata": {},
   "source": [
    "# Lab 9: Clustering\n",
    "\n",
    "This lab explores clustering algorithms including SOM, K-means, Hierarchical Clustering, and DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6be6f2",
   "metadata": {},
   "source": [
    "## Q1: Self Organizing Map (SOM) on Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/JustGlowing/minisom.git\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from minisom import MiniSom\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(f\"Digits dataset shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1027a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to [0,1]; SOM is sensitive to feature scaling\n",
    "X_normalized = X / X.max()\n",
    "\n",
    "# 10x10 grid provides sufficient neurons for digit topology without overfitting\n",
    "som = MiniSom(x=10, y=10, input_len=X_normalized.shape[1], sigma=1.0, learning_rate=0.5)\n",
    "som.random_weights_init(X_normalized)\n",
    "som.train_random(X_normalized, num_iteration=100)\n",
    "\n",
    "print(\"SOM training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31cb195",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = np.array([som.winner(x) for x in X_normalized])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('SOM Clustering Map - Digits Dataset')\n",
    "scatter = plt.scatter(clusters[:, 0], clusters[:, 1], c=y, cmap='tab10', alpha=0.6, s=50)\n",
    "plt.xlabel('SOM Grid X')\n",
    "plt.ylabel('SOM Grid Y')\n",
    "plt.colorbar(scatter, label='Digit Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Heatmap of cluster frequencies\n",
    "plt.subplot(1, 2, 2)\n",
    "cluster_map = np.zeros((10, 10))\n",
    "plt.subplot(1, 2, 2)\n",
    "cluster_map = np.zeros((10, 10))\n",
    "for cluster in clusters:\n",
    "    cluster_map[cluster[0], cluster[1]] += 1\n",
    "plt.imshow(cluster_map, cmap='YlOrRd')\n",
    "plt.title('Cluster Frequency Map')\n",
    "plt.xlabel('SOM Grid X')\n",
    "plt.ylabel('SOM Grid Y')\n",
    "plt.colorbar(label='Number of Samples')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b4ac7",
   "metadata": {},
   "source": [
    "## Q2: Clustering Comparison - K-means, Hierarchical, DBSCAN on Wine Quality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc72ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine_data = pd.read_csv(url, sep=';')\n",
    "\n",
    "print(f\"Wine dataset shape: {wine_data.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(wine_data.head())\n",
    "print(f\"\\nColumn names:\")\n",
    "print(wine_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358865af",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = wine_data.dropna()\n",
    "\n",
    "X_wine = wine_data.drop('quality', axis=1)\n",
    "y_wine = wine_data['quality']\n",
    "\n",
    "# Distance based algorithms require normalized scales to avoid feature dominance\n",
    "scaler = StandardScaler()\n",
    "X_wine_scaled = scaler.fit_transform(X_wine)\n",
    "\n",
    "print(f\"Scaled data shape: {X_wine_scaled.shape}\")\n",
    "print(f\"Quality distribution:\")\n",
    "print(y_wine.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba313a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"K-MEANS CLUSTERING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "k_values = [2, 3, 4]\n",
    "kmeans_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    # n_init=10 mitigates random initialization sensitivity\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_labels = kmeans.fit_predict(X_wine_scaled)\n",
    "    \n",
    "    silhouette = silhouette_score(X_wine_scaled, kmeans_labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_wine_scaled, kmeans_labels)\n",
    "    inertia = kmeans.inertia_\n",
    "    \n",
    "    kmeans_results[k] = {\n",
    "        'labels': kmeans_labels,\n",
    "        'silhouette': silhouette,\n",
    "        'davies_bouldin': davies_bouldin,\n",
    "        'inertia': inertia\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nK = {k}:\")\n",
    "    print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"  Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "    print(f\"  Inertia (Sum of Squared Distances): {inertia:.2f}\")\n",
    "    print(f\"  Cluster sizes: {np.bincount(kmeans_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HIERARCHICAL CLUSTERING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hierarchical_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    # Ward linkage minimizes within-cluster variance, preferred for exploratory analysis\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "    hier_labels = hierarchical.fit_predict(X_wine_scaled)\n",
    "    \n",
    "    silhouette = silhouette_score(X_wine_scaled, hier_labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_wine_scaled, hier_labels)\n",
    "    \n",
    "    hierarchical_results[k] = {\n",
    "        'labels': hier_labels,\n",
    "        'silhouette': silhouette,\n",
    "        'davies_bouldin': davies_bouldin\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nK = {k}:\")\n",
    "    print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"  Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "    print(f\"  Cluster sizes: {np.bincount(hier_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05483aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DBSCAN CLUSTERING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eps_values = [0.5, 0.75, 1.0]\n",
    "min_samples = 5\n",
    "dbscan_results = {}\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan_labels = dbscan.fit_predict(X_wine_scaled)\n",
    "    \n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    n_noise = list(dbscan_labels).count(-1)\n",
    "    \n",
    "    # Silhouette requires at least 2 clusters and doesn't work well with >50% noise\n",
    "    if n_clusters > 1 and n_noise < len(dbscan_labels) * 0.5:\n",
    "        silhouette = silhouette_score(X_wine_scaled, dbscan_labels)\n",
    "    else:\n",
    "        silhouette = -1\n",
    "    \n",
    "    dbscan_results[eps] = {\n",
    "        'labels': dbscan_labels,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'silhouette': silhouette\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEps = {eps}:\")\n",
    "    print(f\"  Number of Clusters: {n_clusters}\")\n",
    "    print(f\"  Number of Noise Points: {n_noise}\")\n",
    "    if silhouette > 0:\n",
    "        print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "    else:\n",
    "        print(f\"  Silhouette Score: N/A (too many noise points)\")\n",
    "    print(f\"  Cluster sizes: {np.bincount(dbscan_labels[dbscan_labels >= 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dcb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA projection for visual comparison, captures ~65% of wine dataset variance in 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_wine_2d = pca.fit_transform(X_wine_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# K-means (k=3)\n",
    "ax = axes[0, 0]\n",
    "scatter = ax.scatter(X_wine_2d[:, 0], X_wine_2d[:, 1], c=kmeans_results[3]['labels'], \n",
    "                      cmap='viridis', alpha=0.6, s=50)\n",
    "ax.set_title('K-Means (k=3)\\nSilhouette: {:.4f}'.format(kmeans_results[3]['silhouette']), fontsize=12)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "\n",
    "# Hierarchical (k=3)\n",
    "ax = axes[0, 1]\n",
    "scatter = ax.scatter(X_wine_2d[:, 0], X_wine_2d[:, 1], c=hierarchical_results[3]['labels'], \n",
    "                      cmap='viridis', alpha=0.6, s=50)\n",
    "ax.set_title('Hierarchical Clustering (k=3)\\nSilhouette: {:.4f}'.format(hierarchical_results[3]['silhouette']), fontsize=12)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "\n",
    "# DBSCAN (eps=0.5)\n",
    "ax = axes[1, 0]\n",
    "scatter = ax.scatter(X_wine_2d[:, 0], X_wine_2d[:, 1], c=dbscan_results[0.5]['labels'], \n",
    "                      cmap='viridis', alpha=0.6, s=50)\n",
    "ax.set_title('DBSCAN (eps=0.5)\\nClusters: {}, Noise: {}'.format(\n",
    "    dbscan_results[0.5]['n_clusters'], dbscan_results[0.5]['n_noise']), fontsize=12)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "\n",
    "# Silhouette comparison\n",
    "ax = axes[1, 1]\n",
    "k_vals = [2, 3, 4]\n",
    "kmeans_sil = [kmeans_results[k]['silhouette'] for k in k_vals]\n",
    "hier_sil = [hierarchical_results[k]['silhouette'] for k in k_vals]\n",
    "\n",
    "x = np.arange(len(k_vals))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, kmeans_sil, width, label='K-Means', alpha=0.8)\n",
    "ax.bar(x + width/2, hier_sil, width, label='Hierarchical', alpha=0.8)\n",
    "ax.set_xlabel('Number of Clusters (k)')\n",
    "ax.set_ylabel('Silhouette Score')\n",
    "ax.set_title('Silhouette Score Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(k_vals)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c90e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HIERARCHICAL CLUSTERING DENDROGRAM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dendrogram becomes unreadable with >100 samples; sample subset for visualization\n",
    "sample_indices = np.random.choice(X_wine_scaled.shape[0], size=50, replace=False)\n",
    "X_sample = X_wine_scaled[sample_indices]\n",
    "\n",
    "linkage_matrix = linkage(X_sample, method='ward')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title('Hierarchical Clustering Dendrogram (50 sample subset)', fontsize=14)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.axhline(y=20, color='r', linestyle='--', label='Cut for k=4')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5cf815",
   "metadata": {},
   "source": [
    "## Summary and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8958b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY AND DISCUSSION OF RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. SOM (Self-Organizing Map):\")\n",
    "print(\"-\" * 70)\n",
    "print(\"   - Successfully clustered the digits dataset using a 10x10 SOM grid\")\n",
    "print(f\"   - Utilized {len(np.unique(clusters, axis=0))} unique clusters from the 100-node grid\")\n",
    "print(\"   - SOM is useful for topology-preserving dimensionality reduction\")\n",
    "print(\"   - Good for visualizing high-dimensional data in a 2D/3D space\")\n",
    "\n",
    "print(\"\\n2. K-Means Clustering:\")\n",
    "print(\"-\" * 70)\n",
    "best_k_kmeans = max(kmeans_results.keys(), key=lambda k: kmeans_results[k]['silhouette'])\n",
    "print(f\"   - Best k value: {best_k_kmeans} (Silhouette: {kmeans_results[best_k_kmeans]['silhouette']:.4f})\")\n",
    "print(\"   - Advantages: Fast, scalable, easy to understand\")\n",
    "print(\"   - Disadvantages: Requires specifying k in advance, sensitive to initial centroids\")\n",
    "print(\"   - Uses Euclidean distance for cluster assignment\")\n",
    "\n",
    "print(\"\\n3. Hierarchical Clustering (Ward linkage):\")\n",
    "print(\"-\" * 70)\n",
    "best_k_hier = max(hierarchical_results.keys(), key=lambda k: hierarchical_results[k]['silhouette'])\n",
    "print(f\"   - Best k value: {best_k_hier} (Silhouette: {hierarchical_results[best_k_hier]['silhouette']:.4f})\")\n",
    "print(\"   - Advantages: Produces dendrogram for visualization, no need to specify k beforehand\")\n",
    "print(\"   - Disadvantages: Computationally expensive for large datasets, sensitive to outliers\")\n",
    "print(\"   - Used Ward linkage to minimize within-cluster variance\")\n",
    "\n",
    "print(\"\\n4. DBSCAN:\")\n",
    "print(\"-\" * 70)\n",
    "best_eps = 0.5\n",
    "print(f\"   - Best eps value: {best_eps} (Clusters: {dbscan_results[best_eps]['n_clusters']}, Noise: {dbscan_results[best_eps]['n_noise']})\")\n",
    "print(\"   - Advantages: No need to specify k, can find arbitrary-shaped clusters, identifies outliers\")\n",
    "print(\"   - Disadvantages: Sensitive to eps and min_samples parameters, struggles with varying densities\")\n",
    "print(\"   - Density-based approach for finding clusters of arbitrary shapes\")\n",
    "\n",
    "print(\"\\n5. Comparison:\")\n",
    "print(\"-\" * 70)\n",
    "silh_comparison = {\n",
    "    'K-Means (k=3)': kmeans_results[3]['silhouette'],\n",
    "    'Hierarchical (k=3)': hierarchical_results[3]['silhouette'],\n",
    "    'DBSCAN (eps=0.5)': dbscan_results[0.5]['silhouette'] if dbscan_results[0.5]['silhouette'] > 0 else None\n",
    "}\n",
    "for method, score in silh_comparison.items():\n",
    "    if score is not None:\n",
    "        print(f\"   {method}: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"   {method}: N/A\")\n",
    "\n",
    "print(\"\\n6. Recommendations:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"   - K-Means is recommended for large datasets when k is known\")\n",
    "print(\"   - Hierarchical clustering is good for exploratory analysis and small-medium datasets\")\n",
    "print(\"   - DBSCAN is best when expecting clusters of varying densities and unknown k\")\n",
    "print(\"   - Consider using ensemble methods for more robust clustering results\")\n",
    "print(\"   - Always standardize features before clustering (especially for distance-based methods)\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
